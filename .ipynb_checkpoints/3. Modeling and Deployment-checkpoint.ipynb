{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b71dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import uniform\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "import time\n",
    "#libraries for preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c5fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "df = pd.read_csv(\"data/bank_enriched_data.csv\", sep = ';')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb1cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Display of column name and data type\n",
    "df = df.drop('Unnamed: 0',axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108801e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1fc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split df to X and Y\n",
    "y = df.loc[:, 'y'].values\n",
    "X = df.drop('y', axis=1)\n",
    "\n",
    "# split data into 80-20 for training set / test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c7c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_normalized_arr=scaler.transform(X_train)\n",
    "X_train_normalized_df=pd.DataFrame(X_train_normalized_arr, columns=list(X.columns))\n",
    "\n",
    "X_test_normalized_arr=scaler.transform(X_test)\n",
    "X_test_normalized_df=pd.DataFrame(X_test_normalized_arr, columns=list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f1642",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1bb4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab2c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_normalized_df size is\", len(X_train_normalized_df))\n",
    "print(\"----------------------------------\")\n",
    "print(\"X_test_normalized_df size is\", len(X_test_normalized_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fcee25",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa172437",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 500, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc27002",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train_normalized_df, y_train)\n",
    "rf_y_pred = rf.predict_proba(X_test_normalized_df)\n",
    "\n",
    "pd.Series(rf.feature_importances_, index = X_train_normalized_df.columns).nlargest(15).plot(kind = 'pie',\n",
    "                                                                               figsize = (24, 12),\n",
    "                                                                              title = 'Feature importance from RandomForest', ylabel='', colormap='tab20c')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2f722",
   "metadata": {},
   "source": [
    "## Permutation Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10742ae4",
   "metadata": {},
   "source": [
    "##### - This iteration may overfit but we'll find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b555eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx = result.importances_mean.argsort()\n",
    "importances = pd.DataFrame(\n",
    "    result.importances[sorted_importances_idx].T,\n",
    "    columns=X.columns[sorted_importances_idx],\n",
    ")\n",
    "ax = importances.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ecf317",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "sorted_importances_idx = result.importances_mean.argsort()\n",
    "importances = pd.DataFrame(\n",
    "    result.importances[sorted_importances_idx].T,\n",
    "    columns=X.columns[sorted_importances_idx],\n",
    ")\n",
    "ax = importances.plot.box(vert=False, whis=10)\n",
    "ax.set_title(\"Permutation Importances (train set)\")\n",
    "ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cf276c",
   "metadata": {},
   "source": [
    "`````- We can further retry the experiment by limiting the capacity of the trees to overfit `````\n",
    "\n",
    "`````- by setting min_samples_leaf at 20 data points`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.set_params(min_samples_leaf=20).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5246a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\n",
    "print(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20495b7",
   "metadata": {},
   "source": [
    "`````- Observing the accuracy score on the training and testing set, we observe that the two metrics are very similar now. `````\n",
    "\n",
    "`````- Therefore, our model is not overfitting anymore. We can then check the permutation importances with this new model. `````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c14682",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = permutation_importance(\n",
    "    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "test_results = permutation_importance(\n",
    "    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_importances_idx = train_result.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_importances = pd.DataFrame(\n",
    "    train_result.importances[sorted_importances_idx].T,\n",
    "    columns=X.columns[sorted_importances_idx],\n",
    ")\n",
    "test_importances = pd.DataFrame(\n",
    "    test_results.importances[sorted_importances_idx].T,\n",
    "    columns=X.columns[sorted_importances_idx],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a968c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, importances in zip([\"train\", \"test\"], [train_importances, test_importances]):\n",
    "    ax = importances.plot.box(vert=False, whis=10)\n",
    "    ax.set_title(f\"Permutation Importances ({name} set)\")\n",
    "    ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "    ax.figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b72ef8",
   "metadata": {},
   "source": [
    "### Sequential Feature Selection Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d53cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeCV(alphas=np.logspace(-6, 6, num=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e8c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns\n",
    "n_features = X.columns.shape[0]-1\n",
    "\n",
    "tic_fwd = time.time()\n",
    "sfs_forward = SequentialFeatureSelector(\n",
    "    ridge, n_features_to_select=n_features, direction=\"forward\"\n",
    ").fit(X, y)\n",
    "toc_fwd = time.time()\n",
    "\n",
    "tic_bwd = time.time()\n",
    "sfs_backward = SequentialFeatureSelector(\n",
    "    ridge, n_features_to_select=n_features, direction=\"backward\"\n",
    ").fit(X, y)\n",
    "toc_bwd = time.time()\n",
    "\n",
    "print(\n",
    "    \"Features selected by forward sequential selection: \"\n",
    "    f\"{feature_names[sfs_forward.get_support()]}\"\n",
    ")\n",
    "print(f\"Done in {toc_fwd - tic_fwd:.3f}s\")\n",
    "print(\n",
    "    \"Features selected by backward sequential selection: \"\n",
    "    f\"{feature_names[sfs_backward.get_support()]}\"\n",
    ")\n",
    "print(f\"Done in {toc_bwd - tic_bwd:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc9d5f",
   "metadata": {},
   "source": [
    "`````- It is rather unusal to see the same set of feature selected in \"forward\" and \"backward\" selection. `````\n",
    "\n",
    "`````- This data may have been synthetically produced.`````\n",
    "\n",
    "`````- However \"backward\" selection worked 5x faster over \"forward\" selection.`````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49abf0dc",
   "metadata": {},
   "source": [
    "## Modeling and Deployment (pkl dump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48a805",
   "metadata": {},
   "source": [
    "`````- Define a function to dislplay the test score`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display test scores and return result string and indexes of false samples\n",
    "def display_test_scores(test, pred):\n",
    "    str_out = \"\\n\"\n",
    "    str_out += (\"#####  TEST SCORES  #####\\n--------------------\")\n",
    "    str_out += (\"\\n\")\n",
    "\n",
    "    #print accuracy\n",
    "    accuracy = accuracy_score(test, pred)\n",
    "    str_out += (\"ACCURACY: {:.4f}\\n\".format(accuracy))\n",
    "    str_out += (\"\\n\")\n",
    "\n",
    "    #print AUC score\n",
    "    auc = roc_auc_score(test, pred)\n",
    "    str_out += (\"AUC: {:.4f}\\n\".format(auc))\n",
    "    str_out += (\"\\n\")\n",
    "\n",
    "    #print confusion matrix\n",
    "    str_out += (\"CONFUSION MATRIX:\\n--------------------\\n\")\n",
    "    conf_mat = confusion_matrix(test, pred)\n",
    "    str_out += (\"{}\".format(conf_mat))\n",
    "    str_out += (\"\\n\")\n",
    "    str_out += (\"\\n--------------------\\n\")\n",
    "\n",
    "    #print classification report\n",
    "    str_out += (\"{}\".format(classification_report(test, pred)))\n",
    "    \n",
    "    false_indexes = np.where(test != pred)\n",
    "    return str_out, false_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b5a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also Define a dataframe to consolidate the results\n",
    "accu=pd.DataFrame(index=['MSLE', 'Root MSLE', 'R2 Score','Accuracy(%)'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe587a",
   "metadata": {},
   "source": [
    "`````Apply various classification methods to a business problem and compare the results of - `````\n",
    "\n",
    "`````- k-nearest neighbors`````\n",
    "\n",
    "`````- logistic regression`````\n",
    "\n",
    "`````- decision trees`````\n",
    "\n",
    "`````- support vector machines`````\n",
    "\n",
    "`````We'll implement all 4 but it is good to assess which algotithm is best for the given dataset. Let's run them thru model selecton method to evaluate that using cross validation method.`````\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of algorithms to consider and set performance measure\n",
    "models = []\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('Logistic Regression', LogisticRegression(solver='liblinear', random_state=7, class_weight='balanced')))\n",
    "models.append(('Decision Tree', DecisionTreeClassifier(max_depth=5)))\n",
    "models.append(('SVM', SVC(gamma='auto', random_state=7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_results = []\n",
    "auc_results = []\n",
    "names = []\n",
    "# set table to table to populate with performance results\n",
    "col = ['Algorithm', 'ROC AUC Mean', 'ROC AUC STD', \n",
    "       'Accuracy Mean', 'Accuracy STD']\n",
    "df_results = pd.DataFrame(columns=col)\n",
    "i = 0\n",
    "# evaluate each model using cross-validation\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(\n",
    "        n_splits=10)  # 10-fold cross-validation\n",
    "\n",
    "    cv_acc_results = model_selection.cross_val_score(  # accuracy scoring\n",
    "        model, X_train_normalized_df, y_train, cv=kfold, scoring='accuracy')\n",
    "\n",
    "    cv_auc_results = model_selection.cross_val_score(  # roc_auc scoring\n",
    "        model, X_train_normalized_df, y_train, cv=kfold, scoring='roc_auc')\n",
    "\n",
    "    acc_results.append(cv_acc_results)\n",
    "    auc_results.append(cv_auc_results)\n",
    "    names.append(name)\n",
    "    df_results.loc[i] = [name,\n",
    "                         round(cv_auc_results.mean()*100, 2),\n",
    "                         round(cv_auc_results.std()*100, 2),\n",
    "                         round(cv_acc_results.mean()*100, 2),\n",
    "                         round(cv_acc_results.std()*100, 2)\n",
    "                         ]\n",
    "    i += 1\n",
    "df_results.sort_values(by=['ROC AUC Mean'], ascending=False)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the results to a file\n",
    "df_results.to_csv('results/model_performance_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 8))\n",
    "fig.suptitle('Algorithm Accuracy Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(acc_results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d9d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 8))\n",
    "fig.suptitle('Algorithm ROC AUC Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(auc_results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53952dc1",
   "metadata": {},
   "source": [
    "#### 1. K-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a833f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e65d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_size = list(range(1,5))\n",
    "n_neighbors = list(range(1,3))\n",
    "p=[1,2]\n",
    "parameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "\n",
    "start_time=time.time()##\n",
    "\n",
    "# grid search for parameters\n",
    "grid_knn = GridSearchCV(estimator=knn, param_grid=parameters)\n",
    "grid_knn.fit(X_train_normalized_df, y_train)\n",
    "\n",
    "pickle.dump(knn, open('models/kneighbor.pkl', 'wb'))\n",
    "\n",
    "# print best scores\n",
    "print(\"The best parameters are %s with a score of %0.4f\\n\"\n",
    "      % (grid_knn.best_params_, grid_knn.best_score_))\n",
    "\n",
    "# prediction results\n",
    "y_pred = grid_knn.predict(X_test_normalized_df)\n",
    "\n",
    "\n",
    "end_time=time.time()##\n",
    "krun_time = end_time-start_time\n",
    "print(\"\\nRun time for train & test cv KNN : \", krun_time)\n",
    "\n",
    "# print accuracy metrics\n",
    "results, false = display_test_scores(y_test, y_pred)\n",
    "print(results)\n",
    "\n",
    "print('='*20)\n",
    "print(\"estimator : \" + str(grid_knn.best_estimator_))\n",
    "print(\"best params: \" + str(grid_knn.best_params_))\n",
    "print('best score:', grid_knn.best_score_)\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d33eb",
   "metadata": {},
   "source": [
    "#### 2. Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642454f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='liblinear', class_weight=\"balanced\", random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba9716",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C': np.arange(1e-03, 2, 0.01)}\n",
    "scoring = {\"AUC\": \"roc_auc\", \"Accuracy\": make_scorer(accuracy_score)}\n",
    "\n",
    "start_time=time.time()##\n",
    "\n",
    "# grid search for parameters\n",
    "grid_lr = GridSearchCV(estimator=lr, param_grid=parameters, scoring=scoring, refit=\"AUC\", n_jobs=2, return_train_score=True, cv=5)\n",
    "grid_lr.fit(X_train_normalized_df, y_train)\n",
    "\n",
    "pickle.dump(lr, open('models/linear_regression.pkl', 'wb'))\n",
    "\n",
    "# print best scores\n",
    "print(\"The best parameters are %s with a score of %0.4f\\n\"\n",
    "      % (grid_lr.best_params_, grid_lr.best_score_))\n",
    "\n",
    "# prediction results\n",
    "y_pred = grid_lr.predict(X_test_normalized_df)\n",
    "\n",
    "\n",
    "end_time=time.time()##\n",
    "lrrun_time = end_time-start_time\n",
    "print(\"\\nRun time for train & test cv Logistic Regression Classifier : \", lrrun_time)\n",
    "\n",
    "# print accuracy metrics\n",
    "results, false = display_test_scores(y_test, y_pred)\n",
    "print(results)\n",
    "\n",
    "print('='*20)\n",
    "print(\"estimator : \" + str(grid_lr.best_estimator_))\n",
    "print(\"best params: \" + str(grid_lr.best_params_))\n",
    "print('best score:', grid_lr.best_score_)\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d396224",
   "metadata": {},
   "source": [
    "#### 3. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60cfee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145743a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={\"min_samples_split\": range(2, 403, 20)}\n",
    "scoring = {\"AUC\": \"roc_auc\", \"Accuracy\": make_scorer(accuracy_score)}\n",
    "\n",
    "start_time=time.time()##\n",
    "\n",
    "# grid search for parameters\n",
    "grid_dtc = GridSearchCV(estimator=dtc, param_grid=parameters, scoring=scoring, refit=\"AUC\", n_jobs=2, return_train_score=True)\n",
    "grid_dtc.fit(X_train_normalized_df, y_train)\n",
    "\n",
    "pickle.dump(dtc, open('models/decison_tree.pkl', 'wb'))\n",
    "\n",
    "# print best scores\n",
    "print(\"The best parameters are %s with a score of %0.4f\\n\"\n",
    "      % (grid_dtc.best_params_, grid_dtc.best_score_))\n",
    "\n",
    "# prediction results\n",
    "y_pred = grid_dtc.predict(X_test_normalized_df)\n",
    "\n",
    "\n",
    "end_time=time.time()##\n",
    "dtrun_time = end_time-start_time\n",
    "print(\"\\nRun time for train & test cv Decision Tree Classifier : \", dtrun_time)\n",
    "\n",
    "# print accuracy metrics\n",
    "results, false = display_test_scores(y_test, y_pred)\n",
    "print(results)\n",
    "\n",
    "print('='*20)\n",
    "print(\"estimator : \" + str(grid_dtc.best_estimator_))\n",
    "print(\"best params: \" + str(grid_dtc.best_params_))\n",
    "print('best score:', grid_dtc.best_score_)\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7634bc98",
   "metadata": {},
   "source": [
    "#### 4. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d684417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters \n",
    "parameters = {\n",
    "                'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                'max_iter': [10, 30, 80, 100, 120],\n",
    "                'class_weight': [None, 'balanced']\n",
    "            }\n",
    "\n",
    "start_time=time.time()##\n",
    "\n",
    "# grid search for parameters\n",
    "grid_svc = GridSearchCV(estimator=svm, param_grid=parameters)\n",
    "grid_svc.fit(X_train_normalized_df, y_train)\n",
    "\n",
    "pickle.dump(svm, open('models/support_vector_machine.pkl', 'wb'))\n",
    "\n",
    "# print best scores\n",
    "print(\"The best parameters are %s with a score of %0.4f\\n\"\n",
    "      % (grid_svc.best_params_, grid_svc.best_score_))\n",
    "\n",
    "# prediction results\n",
    "y_pred = grid_svc.predict(X_test_normalized_df)\n",
    "\n",
    "\n",
    "end_time=time.time()##\n",
    "svcrun_time = end_time-start_time\n",
    "print(\"\\nRun time for train & test cv SVM : \", svcrun_time)\n",
    "\n",
    "# print accuracy metrics\n",
    "results, false = display_test_scores(y_test, y_pred)\n",
    "print(results)\n",
    "\n",
    "print('='*20)\n",
    "print(\"estimator : \" + str(grid_svc.best_estimator_))\n",
    "print(\"best params: \" + str(grid_svc.best_params_))\n",
    "print('best score:', grid_svc.best_score_)\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b039a2e4",
   "metadata": {},
   "source": [
    "#### Results consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "    'Model':['KNN', 'Logistics Regression', 'Decision Tree', 'SVC'],\n",
    "    'Accuracy': [grid_knn.best_score_*100, grid_lr.best_score_*100, grid_dtc.best_score_*100, grid_svc.best_score_*100],\n",
    "    'Train-test run time in Seconds': [krun_time, lrrun_time, dtrun_time, svcrun_time],\n",
    "    'Best estimator': [grid_knn.best_estimator_, grid_lr.best_estimator_, grid_dtc.best_estimator_, grid_svc.best_estimator_]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_dict).set_index('Model')\n",
    "results_df.to_csv('results/model_performance_with_estimators.csv')\n",
    "results_df = results_df.style.set_properties(**{'text-align': 'left'})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82215a4",
   "metadata": {},
   "source": [
    "#### Prediction and Confusion Metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e718072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_head_knn = grid_knn.predict(X_test_normalized_df)\n",
    "y_head_svm = grid_svc.predict(X_test_normalized_df)\n",
    "y_head_dt = grid_dtc.predict(X_test_normalized_df)\n",
    "y_head_lr = grid_lr.predict(X_test_normalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fad2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_knn = confusion_matrix(y_test,y_head_knn)\n",
    "cm_svm = confusion_matrix(y_test,y_head_svm)\n",
    "cm_dt = confusion_matrix(y_test,y_head_dt)\n",
    "cm_lr = confusion_matrix(y_test,y_head_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,12))\n",
    "\n",
    "plt.suptitle(\"Confusion Matrices\",fontsize=12)\n",
    "plt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"Logistic Regression Confusion Matrix\\n\")\n",
    "sns.heatmap(cm_lr,annot=True,cmap=\"Blues\", fmt=\"d\",cbar=False, annot_kws={\"size\": 20})\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"K- Nearest Neighbors Confusion Matrix\\n\")\n",
    "sns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 20})\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.title(\"Support Vector Machine Confusion Matrix\\n\")\n",
    "sns.heatmap(cm_svm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 20})\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.title(\"Decision Tree Confusion Matrix\\n\")\n",
    "sns.heatmap(cm_dt,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 20})\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1177cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accu=pd.read_csv('results/model_performance_with_estimators.csv', index_col=0)\n",
    "accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ff1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_accuracy = accu.T.iloc[0]\n",
    "model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646134a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list(range(len(model_accuracy)))\n",
    "y=list(range(0,101,10))\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=0.8)\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(model_accuracy)\n",
    "plt.yticks(y)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.xticks(rotation = (10))\n",
    "plt.xlabel(\"Models\",fontsize=30)\n",
    "plt.ylabel(\"Accuracy(%)\",fontsize=30)\n",
    "plt.title(\"Performance of Models\")\n",
    "for a,b in zip(x,y):\n",
    "    b=model_accuracy[a]\n",
    "    val=\"(\"+str(round(model_accuracy[a],2))+\" %)\"\n",
    "    plt.text(a, b+4.5, val,horizontalalignment='center',verticalalignment='center',color='green',bbox=props)\n",
    "    plt.text(a, b+3.5, '.',horizontalalignment='center',verticalalignment='center',color='red',fontsize=50)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/Overall_Performance.png',dpi=600)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fbf105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
