{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936bde3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a50427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import max as sparkMax\n",
    "from IPython.display import display\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import warnings; warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7fe29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession \\\n",
    "  .builder \\\n",
    "  .master(\"local[*]\")\\\n",
    "  .appName(\"Pyspark\") \\\n",
    "  .config(\"spark.memory.fraction\", 0.8) \\\n",
    "  .config(\"spark.executor.memory\", \"16g\") \\\n",
    "  .config(\"spark.driver.memory\", \"16g\")\\\n",
    "  .config(\"spark.sql.shuffle.partitions\" , \"800\") \\\n",
    "  .config(\"spark.memory.offHeap.enabled\",'true')\\\n",
    "  .config(\"spark.memory.offHeap.size\",\"16g\")\\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Spark session\n",
    "mySpark = SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession(mySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "data = spark.read.csv(\"marketing/data/bank_full_raw.csv\", sep = ';', inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ee380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display of column name and data type\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f7eff",
   "metadata": {},
   "source": [
    "# 1. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d60d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c5d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of rows & columns with NaN/Null values and duplicates\n",
    "data.dropna()\n",
    "data.dropDuplicates()\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357622a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the null column count using sql\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "data.select([count(when(isnan(a) | col(a).isNull(), a)).alias(a) for a in data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef2aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data\n",
    "df = df.replace('yes','1')\n",
    "df = df.replace('no','0')\n",
    "df = df.withColumn(\"default\",df.default.cast('integer'))\n",
    "df = df.withColumn(\"loan\",df.loan.cast('integer'))\n",
    "df = df.withColumn(\"housing\",df.housing.cast('integer'))\n",
    "df = df.withColumn(\"y\",df.y.cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b2246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display of column name and data type\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupBy(\"month\").count().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a653420",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupBy(\"job\").count().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupBy(\"marital\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f923c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupBy(\"education\").count().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47063954",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupBy(\"housing\").count().show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3090b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupBy(\"loan\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupBy(\"contact\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e1786",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupBy(\"campaign\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47897ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupBy(\"poutcome\").count().show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupBy(\"y\").count().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filter((F.col('poutcome')=='unknown'))\\\n",
    "    .filter(\n",
    "        (F.col('y') == 'yes')\n",
    "    )\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d99873",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filter((F.col('poutcome')=='success'))\\\n",
    "    .filter(\n",
    "        (F.col('y') == 'yes')\n",
    "    )\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd4d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filter((F.col('poutcome')=='failure'))\\\n",
    "    .filter(\n",
    "        (F.col('y') == 'yes')\n",
    "    )\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ef2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "\n",
    "sns.pairplot(data.toPandas())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba4053",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (15,8)) \n",
    "sns.barplot(x=\"job\", y = \"y\", data = df.toPandas())\n",
    "sns.despine(left = True, bottom = True)\n",
    "ax.set(xlabel='job', ylabel='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (15,8)) \n",
    "sns.barplot(x=\"marital\", y = \"y\", data = df.toPandas())\n",
    "sns.despine(left = True, bottom = True)\n",
    "ax.set(xlabel='marital', ylabel='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747eed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (15,8)) \n",
    "sns.barplot(x=\"education\", y = \"y\", data = df.toPandas())\n",
    "sns.despine(left = True, bottom = True)\n",
    "ax.set(xlabel='education', ylabel='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb053f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (15,8)) \n",
    "sns.barplot(x=\"housing\", y = \"y\", data = df.toPandas())\n",
    "sns.despine(left = True, bottom = True)\n",
    "ax.set(xlabel='housing', ylabel='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da07898b",
   "metadata": {},
   "source": [
    "# 2. Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf8fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d5aee3",
   "metadata": {},
   "source": [
    "#### Encode catagorical features and transform/merge multiple columns into a vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e56284",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "categoricalColumns = ['job', 'marital', 'education', 'contact', 'month', 'poutcome']\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "label_stringIdx = StringIndexer(inputCol = 'y', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "numericCols = ['age', 'default', 'balance', 'housing', 'loan', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"Subscribed\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd5eb3e",
   "metadata": {},
   "source": [
    "#### Notice that the dataframe now has 2 new columns in the beginning - \"label\" and \"Subscribed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41144cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df = pipelineModel.transform(df)\n",
    "selectedCols = ['label', 'Subscribed'] + cols\n",
    "df = df.select(selectedCols)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b5d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e866101",
   "metadata": {},
   "source": [
    "#### 80/20 train/test data split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], seed = 2022)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e693e4",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de50612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'Subscribed', labelCol = 'label', maxIter=15)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7854495",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.sort(lrModel.coefficients)\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta-Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaa26fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = trainingSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8502ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(test)\n",
    "predictions.filter((F.col('prediction') == 1.0))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.filter((F.col('prediction') == 1.0))\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db98dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b222ce",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol = 'Subscribed', labelCol = 'label', maxDepth= 3)\n",
    "dtModel = dt.fit(train)\n",
    "predictions = dtModel.transform(test)\n",
    "predictions.filter((F.col('prediction') == 1.0))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93580ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.filter((F.col('prediction') == 1.0))\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be415fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ceb65",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed21cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'Subscribed', labelCol = 'label')\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)\n",
    "predictions.filter((F.col('prediction') == 1.0))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0573766",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd85c3f2",
   "metadata": {},
   "source": [
    "## Gradient-Boosted Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98795ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(featuresCol = 'Subscribed', labelCol = 'label', maxIter=10)\n",
    "gbtModel = gbt.fit(train)\n",
    "predictions = gbtModel.transform(test)\n",
    "predictions.filter((F.col('prediction') == 1.0))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1148ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.filter((F.col('prediction') == 1.0))\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f329031",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2563c92",
   "metadata": {},
   "source": [
    "## Modifying this model with the ParamGridBuilder as well as the CrossValidator because the Logistic Regression produced the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7051f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = (ParamGridBuilder().addGrid(lr.maxIter, [500]) \\\n",
    "                                .addGrid(lr.regParam, [0]) \\\n",
    "                                .addGrid(lr.elasticNetParam, [1]) \\\n",
    "                                .build())\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "cvModel = cv.fit(train)\n",
    "predictions = cvModel.transform(test)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae6c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
