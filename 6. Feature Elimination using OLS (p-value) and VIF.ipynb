{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Ordinary Least Squares (OLS) for - p-value < 0.05\n",
    "#### Find Multicollinearity - keep all the features if the VIF (Variance Inflation Factor) is < 5\n",
    "\n",
    "We will use following algorithms - \n",
    "\n",
    "- **Logistic regression**\n",
    "- **K Nearest Neighbors**\n",
    "- **Support Vector Machine** \n",
    "- **Naive Bayes**\n",
    "- **Decision Tree**\n",
    "- **Random Forest**\n",
    "\n",
    "\n",
    "- **In OLS method**, we have to choose the values of b_1  and b_0  such that, the total sum of squares of the difference between the calculated and observed values of y, is minimized. \n",
    "- **In VIF method** - the outlier influence factor can be eliminated. VIF factor < 5 generally doesn't change the score significantly even if the features are dropped. But VIF factor > 5 can significantly change the score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ****Read Data****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are reading our data\n",
    "df = pd.read_csv(\"marketing/data/bank_full_raw.csv\", delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 10 rows of our data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes('object').columns:\n",
    "    print(col, df[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=preprocessing.LabelEncoder()\n",
    "df[cat_cols]=df[cat_cols].apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[df.columns[df.columns != 'y']]\n",
    "y = df.y\n",
    " \n",
    "# Statsmodels.OLS requires us to add a constant.\n",
    "x = sm.add_constant(x)\n",
    "model = sm.OLS(y,x)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 rows of our data\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Multicollinearity - keep all the features if the VIF (Variance Inflation Factor) is < 5\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "x_temp = sm.add_constant(x_data)\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(x_temp.values, i) for i in range(x_temp.values.shape[1])]\n",
    "vif[\"features\"] = x_temp.columns\n",
    "print(vif.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Colinearity was found bet poutcome [pdays(negative),previous(positive)]**\n",
    "- **Removing those columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values\n",
    "x = x.drop(['const', 'pdays', 'previous', 'poutcome'], axis =1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transpose matrices\n",
    "x_train = x_train.T\n",
    "y_train = y_train.T\n",
    "x_test = x_test.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train.T,y_train.T)\n",
    "acc = lr.score(x_test.T,y_test.T)*100\n",
    "\n",
    "accuracies['Logistic Regression'] = acc\n",
    "print(\"Test Accuracy {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbour Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\n",
    "knn.fit(x_train.T, y_train.T)\n",
    "prediction = knn.predict(x_test.T)\n",
    "\n",
    "print(\"{} KNN Score: {:.2f}%\".format(2, knn.score(x_test.T, y_test.T)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to find best k value to improve our accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to find best k value\n",
    "scoreList = []\n",
    "for i in range(1,25):\n",
    "    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n",
    "    knn2.fit(x_train.T, y_train.T)\n",
    "    scoreList.append(knn2.score(x_test.T, y_test.T))\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(range(1,25), scoreList)\n",
    "plt.xticks(np.arange(1,25,1))\n",
    "plt.xlabel(\"K value\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n",
    "\n",
    "acc = max(scoreList)*100\n",
    "accuracies['KNN'] = acc\n",
    "print(\"Maximum KNN Score is {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(random_state = 1)\n",
    "svm.fit(x_train.T, y_train.T)\n",
    "\n",
    "acc = svm.score(x_test.T,y_test.T)*100\n",
    "accuracies['SVM'] = acc\n",
    "print(\"Test Accuracy of SVM Algorithm: {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(x_train.T, y_train.T)\n",
    "\n",
    "acc = nb.score(x_test.T,y_test.T)*100\n",
    "accuracies['Naive Bayes'] = acc\n",
    "print(\"Accuracy of Naive Bayes: {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(x_train.T, y_train.T)\n",
    "\n",
    "acc = dt.score(x_test.T,y_test.T)*100\n",
    "accuracies['Decision Tree'] = acc\n",
    "print(\"Accuracy of Decision Tree: {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 1)\n",
    "rf.fit(x_train.T, y_train.T)\n",
    "\n",
    "acc = rf.score(x_test.T,y_test.T)*100\n",
    "accuracies['Random Forest'] = acc\n",
    "print(\"Random Forest Algorithm Accuracy Score : {:.2f}%\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"aqua\", \"tan\", \"teal\", \"olive\", \"wheat\", \"salmon\"]\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.yticks(np.arange(0,100,10))\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values\n",
    "y_head_lr = lr.predict(x_test.T)\n",
    "knn3 = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn3.fit(x_train.T, y_train.T)\n",
    "y_head_knn = knn3.predict(x_test.T)\n",
    "y_head_svm = svm.predict(x_test.T)\n",
    "y_head_nb = nb.predict(x_test.T)\n",
    "y_head_dt = dt.predict(x_test.T)\n",
    "y_head_rf = rf.predict(x_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_lr = confusion_matrix(y_test,y_head_lr)\n",
    "cm_knn = confusion_matrix(y_test,y_head_knn)\n",
    "cm_svm = confusion_matrix(y_test,y_head_svm)\n",
    "cm_nb = confusion_matrix(y_test,y_head_nb)\n",
    "cm_dt = confusion_matrix(y_test,y_head_dt)\n",
    "cm_rf = confusion_matrix(y_test,y_head_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,12))\n",
    "\n",
    "plt.suptitle(\"Confusion Matrices\",fontsize=24)\n",
    "plt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"Logistic Regression Confusion Matrix\\n\", fontsize=18)\n",
    "sns.heatmap(cm_lr,annot=True,cmap=\"coolwarm\",fmt=\"d\", linewidths=.5, cbar=False, annot_kws={\"size\": 14})\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"K Nearest Neighbors Confusion Matrix\\n\", fontsize=18)\n",
    "sns.heatmap(cm_knn,annot=True,cmap=\"coolwarm\",fmt=\"d\", linewidths=.5, cbar=False, annot_kws={\"size\": 14})\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.title(\"Support Vector Machine Confusion Matrix\\n\", fontsize=18)\n",
    "sns.heatmap(cm_svm,annot=True,cmap=\"coolwarm\",fmt=\"d\", linewidths=.5, cbar=False, annot_kws={\"size\": 14})\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.title(\"Naive Bayes Confusion Matrix\\n\", fontsize=18)\n",
    "sns.heatmap(cm_nb,annot=True,cmap=\"coolwarm\",fmt=\"d\", linewidths=.5, cbar=False, annot_kws={\"size\": 14})\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.title(\"Decision Tree Confusion Matrix\\n\", fontsize=18)\n",
    "sns.heatmap(cm_dt,annot=True,cmap=\"coolwarm\",fmt=\"d\", linewidths=.5, cbar=False, annot_kws={\"size\": 14})\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.title(\"Random Forest Confusion Matrix\\n\", fontsize=18)\n",
    "sns.heatmap(cm_rf,annot=True,cmap=\"coolwarm\",fmt=\"d\", linewidths=.5, cbar=False, annot_kws={\"size\": 14})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of algorithms to consider and set performance measure\n",
    "from sklearn import model_selection\n",
    "models = []\n",
    "\n",
    "models.append(('Logistic Regression', LogisticRegression()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('SVM', SVC()))\n",
    "models.append(('Naive Bayes', GaussianNB()))\n",
    "models.append(('Decision Tree', DecisionTreeClassifier()))\n",
    "models.append(('Random Forest', RandomForestClassifier()))\n",
    "\n",
    "acc_results = []\n",
    "auc_results = []\n",
    "names = []\n",
    "# set table to table to populate with performance results\n",
    "col = ['Algorithm', 'ROC AUC Mean', 'ROC AUC STD', \n",
    "       'Accuracy Mean', 'Accuracy STD']\n",
    "df_results = pd.DataFrame(columns=col)\n",
    "i = 0\n",
    "# evaluate each model using cross-validation\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(\n",
    "        n_splits=5)  # 10-fold cross-validation\n",
    "\n",
    "    cv_acc_results = model_selection.cross_val_score(  # accuracy scoring\n",
    "        model, x_train.T, y_train.T, cv=kfold, scoring='accuracy')\n",
    "\n",
    "    cv_auc_results = model_selection.cross_val_score(  # roc_auc scoring\n",
    "        model, x_train.T, y_train.T, cv=kfold, scoring='roc_auc')\n",
    "\n",
    "    acc_results.append(cv_acc_results)\n",
    "    auc_results.append(cv_auc_results)\n",
    "    names.append(name)\n",
    "    df_results.loc[i] = [name,\n",
    "                         round(cv_auc_results.mean()*100, 2),\n",
    "                         round(cv_auc_results.std()*100, 2),\n",
    "                         round(cv_acc_results.mean()*100, 2),\n",
    "                         round(cv_acc_results.std()*100, 2)\n",
    "                         ]\n",
    "    i += 1\n",
    "df_results.sort_values(by=['ROC AUC Mean'], ascending=False)\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
